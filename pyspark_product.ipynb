{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOr8jEvrAc10",
        "outputId": "f7521a2a-4e85-48e6-a02b-f771d7571c75"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840625 sha256=c2ee6cc09056ee0d378f28587d9096b32cde663a94d355be3c393a1bfada2513\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install install-jdk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhqmhILTAjcY",
        "outputId": "277f35d6-2b15-4605-dbf9-d5f137e9a246"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting install-jdk\n",
            "  Downloading install_jdk-1.1.0-py3-none-any.whl.metadata (12 kB)\n",
            "Downloading install_jdk-1.1.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: install-jdk\n",
            "Successfully installed install-jdk-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tI-api5bAla6",
        "outputId": "ed14f905-c376-41e6-b752-1ed8c7479ac6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install Faker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umU3ti3tAnii",
        "outputId": "f12c81f9-8853-41ab-83a2-0b82bdc97879"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Faker\n",
            "  Downloading Faker-30.3.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from Faker) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from Faker) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->Faker) (1.16.0)\n",
            "Downloading Faker-30.3.0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Faker\n",
            "Successfully installed Faker-30.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mimesis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFdr6cqsApp_",
        "outputId": "6a560ed2-e811-4d04-e458-027554ba024f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mimesis\n",
            "  Downloading mimesis-18.0.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading mimesis-18.0.0-py3-none-any.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mimesis\n",
            "Successfully installed mimesis-18.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import datetime\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StringType, StructType, StructField\n",
        "from mimesis import Generic\n",
        "from mimesis.locales import Locale\n",
        "fake = Generic(Locale.RU)\n",
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Q-nqNWkA5bV",
        "outputId": "60adeebf-27bf-49c4-912e-8cc03f34b155"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Задание\n",
        "\n",
        "Генерация синтетических данных с использованием Apache Spark\n",
        "Цель задания:\n",
        ">> Использовать\n",
        "- Apache Spark для создания синтетического набора данных, который имитирует информацию о покупках в интернет-магазине.\n",
        "\n",
        "Набор данных должен включать в себя информацию о заказах, включая\n",
        "- дату заказа,\n",
        "- идентификатор пользователя,\n",
        "- название товара,\n",
        "- количество и цену.\n",
        "\n",
        ">> Сгенерированные данные будут использованы для последующего анализа покупательской активности и понимания потребительских трендов.\n",
        "\n",
        "> Шаги выполнения:\n",
        "Генерация данных:\n",
        "\n",
        "Создать DataFrame с полями:\n",
        "- Дата,\n",
        "- UserID,\n",
        "- Продукт,\n",
        "- Количество,\n",
        "- Цена.\n",
        "\n",
        "> - Данные для поля Продукт генерируются из списка возможных товаров ( не меньше 5 товаров )\n",
        "\n",
        "- Количество и Цена должны генерироваться случайно в заданных пределах.\n",
        "\n",
        "> - Дата должна быть в пределах последнего года.\n",
        "\n",
        "- UserID представляет собой случайное число, имитирующее идентификаторы пользователей.\n",
        "\n",
        "> - Обратите внимание, что должна быть возможности изменять количество сгенерированных строк. Минимальное количество - 1000 строк.\n",
        "\n",
        "Сохранение данных:\n",
        "\n",
        "- Сохранить сгенерированный DataFrame в формате CSV для последующего анализа.\n",
        "\n",
        "> - Результат выполнения задания (код генерации синтетических данных и созданный файл *.csv) необходимо выложить в github/gitlab и указать ссылку на Ваш репозиторий (не забудьте: репозиторий должен быть публичным).\n"
      ],
      "metadata": {
        "id": "uwKj_J8l-W_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Список продуктов"
      ],
      "metadata": {
        "id": "qG06W1OCGLcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "product_list = [\"Кукуруза\", \"Редис\", \"Ананас\",\n",
        "             \"Гранат\", \"Персик\", \"Кукла венсдей\",\n",
        "             \"Чермондер синий\", \"Дросель на приору\",\n",
        "             \"очиститель белого\", \"Ккус-Кус\", \"Пахлава\",\n",
        "             \"Че попальная вещь\", \"Радитор печки\", \"Плакат Луффи\",\n",
        "             \"Цинкарь\", \"Арбуз\", \"Хаги ваги\"]"
      ],
      "metadata": {
        "id": "-MkElrV0J3YA"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(datetime.date(random.randint(2024, 2024), random.randint(1, 12), random.randint(1, 28)),\n",
        "         random.randrange(1, 1001, 1),\n",
        "         product_list[random.randrange(len(test_list))],\n",
        "         random.randint(1, 10),\n",
        "         random.randint(100, 100000)) for i in range(2000)]"
      ],
      "metadata": {
        "id": "tplS3z8eEvee"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создать DataFrame с полями:\n",
        "- Дата,\n",
        "- UserID,\n",
        "- Продукт,\n",
        "- Количество,\n",
        "- Цена."
      ],
      "metadata": {
        "id": "IwMMlpYBK323"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Spark SQL Example\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "df = spark.createDataFrame(data, [\"data\", \"UserID\", \"product\", \"quantity\", \"price\"])\n",
        "\n",
        "df.createOrReplaceTempView(\"product\")\n",
        "df.write.format(\"csv\").option(\"header\", \"true\").save(\"path/to/save/file2.csv\")"
      ],
      "metadata": {
        "id": "jGF816QHDmIY"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FdlK8chwAKKF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qDNOx06i_0-_"
      }
    }
  ]
}